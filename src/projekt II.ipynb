{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f429db74",
   "metadata": {},
   "source": [
    "# Import bibliotek\n",
    "\n",
    "Projekt rozpoczniemy od zaimportowania potrzebnych bibliotek: \n",
    "-Tkinter – biblioteka umożliwiająca tworzenie interfejsu graficznego, \n",
    "-cv jest używana do wizji komputerowej w sztucznej inteligencji, uczeniu maszynowym, rozpoznawaniu twarzy, \n",
    "-numpy dodaje możliwość obsługi dużych, wielowymiarowych tabel i macierzy,\n",
    "-tensorflow - wykorzystywana jest w uczeniu maszynowym i głębokich sieciach neuronowych, \n",
    "-keras - zapewnia interfejs Pythona dla sztucznych sieci neuronowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bad77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c0c8a0",
   "metadata": {},
   "source": [
    "# Plik treningowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c681d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a331921",
   "metadata": {},
   "source": [
    "Tworzymy katalogi dla danych treningowych oraz testowych, w nich przechowywać będziemy pliki graficzne przedstawiające wyrazy twarzy. Wykorzystano aż 28709 danych uczących oraz 7178 danych testowych. Tworzymy generator danych obrazu, który wygeneruje zestaw danych z plików graficznych w skali od 1 do 255. Nasze obrazy definiujemy w pikselach, czyli tworzymy tablice, które przechowywują wartości od 1 do 255. Ustawiamy docelowy rozmiar na 48x48 pikseli. Tryb koloru ustawiamy na odcienie szarości, tryb klasowy na kategoryczny, ponieważ będziemy kategoryzować otrzymane dane np. zły, uśmiechnięty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8537b19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'data/train'\n",
    "test_dir = 'data/test'\n",
    "train_gen_data = ImageDataGenerator(rescale=1. / 255)\n",
    "train_test_data = ImageDataGenerator(rescale=1. / 255)\n",
    "train_generator = train_gen_data.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "test_generator = train_gen_data.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79d7cc",
   "metadata": {},
   "source": [
    "Tworzmy model oraz przygotowujemy go do treningu. Zaczynamy od zainicjiwania modelu sekwencyjnego z wykorzystaniem wcześniej zaimportowanej biblioteki keras, takie rozwiązanie umożliwoa tworzenie wartswy po warstwie. Za pomocą metody add modelujemy naszą sieć neuronować dodając poszczególne parametry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e40adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential()\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccde30c",
   "metadata": {},
   "source": [
    "Teraz przyszedł czas na kompilację oraz zapisanie naszego modelu. Na samym kończu zostaną zapisane wagi do pliku model.h5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "870b1978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mateusz\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "C:\\Users\\Mateusz\\AppData\\Local\\Temp\\ipykernel_10556\\1724833173.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  emotion_model.info = emotion_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 251s 559ms/step - loss: 1.7996 - accuracy: 0.2622 - val_loss: 1.7165 - val_accuracy: 0.3504\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 150s 334ms/step - loss: 1.6311 - accuracy: 0.3665 - val_loss: 1.5472 - val_accuracy: 0.4097\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 168s 374ms/step - loss: 1.5317 - accuracy: 0.4120 - val_loss: 1.4666 - val_accuracy: 0.4407\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 170s 381ms/step - loss: 1.4663 - accuracy: 0.4401 - val_loss: 1.4146 - val_accuracy: 0.4671\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 165s 369ms/step - loss: 1.4129 - accuracy: 0.4584 - val_loss: 1.3739 - val_accuracy: 0.4766\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 168s 376ms/step - loss: 1.3590 - accuracy: 0.4820 - val_loss: 1.3346 - val_accuracy: 0.4976\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 180s 401ms/step - loss: 1.3175 - accuracy: 0.5024 - val_loss: 1.3053 - val_accuracy: 0.5033\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 174s 388ms/step - loss: 1.2767 - accuracy: 0.5179 - val_loss: 1.2518 - val_accuracy: 0.5266\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 172s 384ms/step - loss: 1.2424 - accuracy: 0.5312 - val_loss: 1.2279 - val_accuracy: 0.5378\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 170s 378ms/step - loss: 1.2068 - accuracy: 0.5469 - val_loss: 1.2094 - val_accuracy: 0.5458\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 170s 380ms/step - loss: 1.1797 - accuracy: 0.5591 - val_loss: 1.1873 - val_accuracy: 0.5526\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 170s 380ms/step - loss: 1.1462 - accuracy: 0.5683 - val_loss: 1.1777 - val_accuracy: 0.5564\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 174s 388ms/step - loss: 1.1250 - accuracy: 0.5772 - val_loss: 1.1665 - val_accuracy: 0.5593\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 171s 382ms/step - loss: 1.0958 - accuracy: 0.5887 - val_loss: 1.1454 - val_accuracy: 0.5759\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 168s 376ms/step - loss: 1.0715 - accuracy: 0.6014 - val_loss: 1.1567 - val_accuracy: 0.5685\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 170s 379ms/step - loss: 1.0504 - accuracy: 0.6075 - val_loss: 1.1224 - val_accuracy: 0.5788\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 169s 376ms/step - loss: 1.0246 - accuracy: 0.6171 - val_loss: 1.1217 - val_accuracy: 0.5822\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 168s 376ms/step - loss: 1.0044 - accuracy: 0.6258 - val_loss: 1.1112 - val_accuracy: 0.5886\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 165s 368ms/step - loss: 0.9803 - accuracy: 0.6340 - val_loss: 1.1028 - val_accuracy: 0.5907\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 152s 340ms/step - loss: 0.9602 - accuracy: 0.6442 - val_loss: 1.0987 - val_accuracy: 0.5924\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 151s 338ms/step - loss: 0.9320 - accuracy: 0.6563 - val_loss: 1.0838 - val_accuracy: 0.5999\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 151s 338ms/step - loss: 0.9122 - accuracy: 0.6661 - val_loss: 1.0859 - val_accuracy: 0.5991\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 151s 336ms/step - loss: 0.8834 - accuracy: 0.6738 - val_loss: 1.0869 - val_accuracy: 0.6039\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 151s 338ms/step - loss: 0.8627 - accuracy: 0.6821 - val_loss: 1.0719 - val_accuracy: 0.6062\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 152s 340ms/step - loss: 0.8425 - accuracy: 0.6900 - val_loss: 1.0740 - val_accuracy: 0.6041\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 150s 336ms/step - loss: 0.8150 - accuracy: 0.7008 - val_loss: 1.0756 - val_accuracy: 0.6090\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 150s 335ms/step - loss: 0.7885 - accuracy: 0.7120 - val_loss: 1.0902 - val_accuracy: 0.6109\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 150s 336ms/step - loss: 0.7756 - accuracy: 0.7129 - val_loss: 1.0876 - val_accuracy: 0.6083\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 150s 334ms/step - loss: 0.7502 - accuracy: 0.7267 - val_loss: 1.0708 - val_accuracy: 0.6099\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 150s 334ms/step - loss: 0.7239 - accuracy: 0.7348 - val_loss: 1.0951 - val_accuracy: 0.6083\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 150s 334ms/step - loss: 0.6967 - accuracy: 0.7484 - val_loss: 1.0915 - val_accuracy: 0.6130\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 149s 333ms/step - loss: 0.6776 - accuracy: 0.7528 - val_loss: 1.0888 - val_accuracy: 0.6124\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 150s 335ms/step - loss: 0.6604 - accuracy: 0.7567 - val_loss: 1.0920 - val_accuracy: 0.6130\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 148s 331ms/step - loss: 0.6424 - accuracy: 0.7702 - val_loss: 1.0954 - val_accuracy: 0.6166\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 153s 342ms/step - loss: 0.6125 - accuracy: 0.7788 - val_loss: 1.1131 - val_accuracy: 0.6150\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 149s 332ms/step - loss: 0.5978 - accuracy: 0.7831 - val_loss: 1.1138 - val_accuracy: 0.6172\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 150s 336ms/step - loss: 0.5766 - accuracy: 0.7917 - val_loss: 1.1144 - val_accuracy: 0.6099\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 149s 331ms/step - loss: 0.5639 - accuracy: 0.7959 - val_loss: 1.1243 - val_accuracy: 0.6147\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 150s 336ms/step - loss: 0.5363 - accuracy: 0.8058 - val_loss: 1.1266 - val_accuracy: 0.6155\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 151s 336ms/step - loss: 0.5188 - accuracy: 0.8138 - val_loss: 1.1320 - val_accuracy: 0.6198\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 151s 336ms/step - loss: 0.5063 - accuracy: 0.8181 - val_loss: 1.1489 - val_accuracy: 0.6186\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 150s 335ms/step - loss: 0.4876 - accuracy: 0.8238 - val_loss: 1.1513 - val_accuracy: 0.6205\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 169s 378ms/step - loss: 0.4679 - accuracy: 0.8295 - val_loss: 1.1494 - val_accuracy: 0.6177\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 173s 386ms/step - loss: 0.4550 - accuracy: 0.8381 - val_loss: 1.1597 - val_accuracy: 0.6190\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 151s 337ms/step - loss: 0.4440 - accuracy: 0.8397 - val_loss: 1.1690 - val_accuracy: 0.6198\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 150s 335ms/step - loss: 0.4273 - accuracy: 0.8474 - val_loss: 1.1796 - val_accuracy: 0.6197\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 152s 339ms/step - loss: 0.4133 - accuracy: 0.8514 - val_loss: 1.2180 - val_accuracy: 0.6217\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 151s 337ms/step - loss: 0.3972 - accuracy: 0.8574 - val_loss: 1.1952 - val_accuracy: 0.6182\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 153s 341ms/step - loss: 0.3893 - accuracy: 0.8599 - val_loss: 1.2067 - val_accuracy: 0.6205\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 151s 336ms/step - loss: 0.3711 - accuracy: 0.8660 - val_loss: 1.2362 - val_accuracy: 0.6147\n"
     ]
    }
   ],
   "source": [
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "emotion_model.info = emotion_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=28709 // 64,\n",
    "    epochs=50,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=7178 // 64\n",
    ")\n",
    "emotion_model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d95aeb",
   "metadata": {},
   "source": [
    "# Plik emocji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6face350",
   "metadata": {},
   "source": [
    "Do pliku emocji dołączamy model sekwencyjny..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d4dba1",
   "metadata": {},
   "source": [
    "Tworzymy słownik z wszystkich emocji, które planujemy rozpoznawać. Importujemy obrazki, które będą prezentować rozpoznany humor.  Tworzymy zmienne globalne, które będziemy wykorzystywać do czytania klatek po kolei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d95dcbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "cur_path = os.path.abspath(os.curdir)\n",
    "emoji_dist = {0: cur_path + \"/emojis/angry.png\", 1: cur_path + \"/emojis/disgusted.png\",\n",
    "              2: cur_path + \"/emojis/fearful.png\", 3: cur_path + \"/emojis/happy.png\",\n",
    "              4: cur_path + \"/emojis/neutral.png\", 5: cur_path + \"/emojis/sad.png\",\n",
    "              6: cur_path + \"/emojis/surprised.png\"}\n",
    "\n",
    "global last_frame1\n",
    "last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "global cap1\n",
    "show_text = [0]\n",
    "global frame_number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31e601",
   "metadata": {},
   "source": [
    "W pierwszym kroku otwieramy nasze wideo. Następnie obliczamy długość naszych ramek. Ważnym elementem metody jest zmiana ramki z 600 na 500, dzięki temu rozwiązaniu mamy pewność, że ramka została odczytana. Zmienna bounding_box to ramka, która znajduje się obok twarzy osoby z filmiku. Potem konwertujemy obraz z filmiku na odcienie szarości, aby wykorzystać wcześniej przygotowany model klasyfikacji, taka konwersja pozwala nam zaoszczędzić dużo miejsca w pamięci w porównaniu do kolorowych zdjęć. Na samym końcu naszej metody aktualizujemy okno główne, aby uzytkać najnowsze dane. Ważnym krokiem tej metody jest konwersja obrazu na RGB. Na końcu opóźniamy wywyłanie funcji o 10 milisekund."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a38e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_subject():\n",
    "    cap1 = cv2.VideoCapture(r'example2.mp4')\n",
    "    if not cap1.isOpened():\n",
    "        print(\"Can't open the camera\")\n",
    "    global frame_number\n",
    "    length = int(cap1.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_number += 1\n",
    "    if frame_number >= length:\n",
    "        exit()\n",
    "    cap1.set(1, frame_number)\n",
    "    flag1, frame1 = cap1.read()\n",
    "    frame1 = cv2.resize(frame1, (600, 500))\n",
    "    bounding_box = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame1, (x, y - 50), (x + w, y + h + 10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame1, emotion_dict[maxindex], (x + 20, y - 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2,\n",
    "                    cv2.LINE_AA)\n",
    "        show_text[0] = maxindex\n",
    "    if flag1 is None:\n",
    "        print(\"Major error!\")\n",
    "    elif flag1:\n",
    "        global last_frame1\n",
    "        last_frame1 = frame1.copy()\n",
    "        pic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(pic)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        lmain.imgtk = imgtk\n",
    "        lmain.configure(image=imgtk)\n",
    "        root.update()\n",
    "        lmain.after(10, show_subject)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eefd2a7",
   "metadata": {},
   "source": [
    "W tej metodzie odczytujemy wartość emocji, którą chcemy przekonwertować na RGB. Otwwieramy odpowiadający obrazek oraz ładujemy go do etykiety interfejsu. Aktualizujemy katalog główny oraz wywołujemy tą samą funkcję opóźnioną o 10 milisekund."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b30e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_avatar():\n",
    "    frame2 = cv2.imread(emoji_dist[show_text[0]])\n",
    "    pic2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "    img2 = Image.fromarray(frame2)\n",
    "    imgtk2 = ImageTk.PhotoImage(image=img2)\n",
    "    lmain2.imgtk2 = imgtk2\n",
    "    lmain3.configure(text=emotion_dict[show_text[0]], font=('arial', 45, 'bold'))\n",
    "    lmain2.configure(image=imgtk2)\n",
    "    root.update()\n",
    "    lmain2.after(10, show_avatar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53145b23",
   "metadata": {},
   "source": [
    "Inicjujemy ramkę główną oraz potrzebne labele, które będą zawierały załadowe zdjęcia lub film oraz ustawiemy parametru interfejsu programu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "256c4cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Mateusz\\anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\Mateusz\\anaconda3\\lib\\tkinter\\__init__.py\", line 814, in callit\n",
      "    func(*args)\n",
      "  File \"C:\\Users\\Mateusz\\AppData\\Local\\Temp\\ipykernel_10556\\2591147030.py\", line 32, in show_subject\n",
      "    imgtk = ImageTk.PhotoImage(image=img)\n",
      "  File \"C:\\Users\\Mateusz\\anaconda3\\lib\\site-packages\\PIL\\ImageTk.py\", line 112, in __init__\n",
      "    self.__photo = tkinter.PhotoImage(**kw)\n",
      "  File \"C:\\Users\\Mateusz\\anaconda3\\lib\\tkinter\\__init__.py\", line 4064, in __init__\n",
      "    Image.__init__(self, 'photo', name, cnf, master, **kw)\n",
      "  File \"C:\\Users\\Mateusz\\anaconda3\\lib\\tkinter\\__init__.py\", line 3997, in __init__\n",
      "    master = _get_default_root('create image')\n",
      "  File \"C:\\Users\\Mateusz\\anaconda3\\lib\\tkinter\\__init__.py\", line 297, in _get_default_root\n",
      "    raise RuntimeError(f\"Too early to {what}: no default root window\")\n",
      "RuntimeError: Too early to create image: no default root window\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    frame_number = 0\n",
    "    root = tk.Tk()\n",
    "    lmain = tk.Label(master=root, padx=50, bd=10)\n",
    "    lmain2 = tk.Label(master=root, bd=10)\n",
    "    lmain3 = tk.Label(master=root, bd=10, fg=\"#CDCDCD\", bg=\"black\")\n",
    "    lmain.pack(side=LEFT)\n",
    "    lmain.place(x=50, y=250)\n",
    "    lmain3.pack()\n",
    "    lmain3.place(x=960, y=250)\n",
    "    lmain2.pack()\n",
    "    lmain2.place(x=960, y=350)\n",
    "\n",
    "    root.title(\"Photo to Emoji\")\n",
    "    root.geometry(\"1400x900+100+10\")\n",
    "    root['bg'] = 'black'\n",
    "    Button(root, text=\"Quit\", fg=\"red\", command=root.destroy, font=(\"arial\", 25, \"bold\")).pack(side=BOTTOM)\n",
    "    threading.Thread(target=show_subject).start()\n",
    "    threading.Thread(target=show_avatar).start()\n",
    "    root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
