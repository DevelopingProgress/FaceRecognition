{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Rozpoznawanie wyrazu twarzy na podstawie wideo\n",
    "\n",
    "# Import bibliotek\n",
    "\n",
    "Projekt rozpoczniemy od zaimportowania potrzebnych bibliotek: \n",
    "-Tkinter – biblioteka umożliwiająca tworzenie interfejsu graficznego, \n",
    "-cv jest używana do wizji komputerowej w sztucznej inteligencji, uczeniu maszynowym, rozpoznawaniu twarzy, \n",
    "-numpy dodaje możliwość obsługi dużych, wielowymiarowych tabel i macierzy,\n",
    "-tensorflow - wykorzystywana jest w uczeniu maszynowym i głębokich sieciach neuronowych, \n",
    "-keras - zapewnia interfejs Pythona dla sztucznych sieci neuronowych."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plik treningowy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tworzymy katalogi dla danych treningowych oraz testowych, w nich przechowywać będziemy pliki graficzne przedstawiające wyrazy twarzy. Wykorzystano aż 28709 danych uczących oraz 7178 danych testowych. Tworzymy generator danych obrazu, który wygeneruje zestaw danych z plików graficznych w skali od 1 do 255. Nasze obrazy definiujemy w pikselach, czyli tworzymy tablice, które przechowywują wartości od 1 do 255. Ustawiamy docelowy rozmiar na 48x48 pikseli. Tryb koloru ustawiamy na odcienie szarości, tryb klasowy na kategoryczny, ponieważ będziemy kategoryzować otrzymane dane np. zły, uśmiechnięty. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dir = 'data/train'\n",
    "test_dir = 'data/test'\n",
    "train_gen_data = ImageDataGenerator(rescale=1. / 255)\n",
    "train_test_data = ImageDataGenerator(rescale=1. / 255)\n",
    "train_generator = train_gen_data.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "test_generator = train_gen_data.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tworzmy model oraz przygotowujemy go do treningu. Zaczynamy od zainicjiwania modelu sekwencyjnego z wykorzystaniem wcześniej zaimportowanej biblioteki keras, takie rozwiązanie umożliwoa tworzenie wartswy po warstwie. Za pomocą metody add modelujemy naszą sieć neuronować dodając poszczególne parametry."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "emotion_model = Sequential()\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Teraz przyszedł czas na kompilację oraz zapisanie naszego modelu. Na samym kończu zostaną zapisane wagi do pliku model.h5."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "emotion_model.info = emotion_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=28709 // 64,\n",
    "    epochs=50,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=7178 // 64\n",
    ")\n",
    "emotion_model.save_weights('model.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plik emocji"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do pliku emocji dołączamy model sekwencyjny..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tworzymy słownik z wszystkich emocji, które planujemy rozpoznawać. Importujemy obrazki, które będą prezentować rozpoznany humor.  Tworzymy zmienne globalne, które będziemy wykorzystywać do czytania klatek po kolei."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "cur_path = os.path.abspath(os.curdir)\n",
    "emoji_dist = {0: cur_path + \"/emojis/angry.png\", 1: cur_path + \"/emojis/disgusted.png\",\n",
    "              2: cur_path + \"/emojis/fearful.png\", 3: cur_path + \"/emojis/happy.png\",\n",
    "              4: cur_path + \"/emojis/neutral.png\", 5: cur_path + \"/emojis/sad.png\",\n",
    "              6: cur_path + \"/emojis/surprised.png\"}\n",
    "\n",
    "global last_frame1\n",
    "last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "global cap1\n",
    "show_text = [0]\n",
    "global frame_number"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "W pierwszym kroku otwieramy nasze wideo. Następnie obliczamy długość naszych ramek. Ważnym elementem metody jest zmiana ramki z 600 na 500, dzięki temu rozwiązaniu mamy pewność, że ramka została odczytana. Zmienna bounding_box to ramka, która znajduje się obok twarzy osoby z filmiku. Potem konwertujemy obraz z filmiku na odcienie szarości, aby wykorzystać wcześniej przygotowany model klasyfikacji, taka konwersja pozwala nam zaoszczędzić dużo miejsca w pamięci w porównaniu do kolorowych zdjęć. Na samym końcu naszej metody aktualizujemy okno główne, aby uzytkać najnowsze dane. Ważnym krokiem tej metody jest konwersja obrazu na RGB. Na końcu opóźniamy wywyłanie funcji o 10 milisekund."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_subject():\n",
    "    cap1 = cv2.VideoCapture(r'example2.mp4')\n",
    "    if not cap1.isOpened():\n",
    "        print(\"Can't open the camera\")\n",
    "    global frame_number\n",
    "    length = int(cap1.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_number += 1\n",
    "    if frame_number >= length:\n",
    "        exit()\n",
    "    cap1.set(1, frame_number)\n",
    "    flag1, frame1 = cap1.read()\n",
    "    frame1 = cv2.resize(frame1, (600, 500))\n",
    "    bounding_box = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame1, (x, y - 50), (x + w, y + h + 10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame1, emotion_dict[maxindex], (x + 20, y - 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2,\n",
    "                    cv2.LINE_AA)\n",
    "        show_text[0] = maxindex\n",
    "    if flag1 is None:\n",
    "        print(\"Major error!\")\n",
    "    elif flag1:\n",
    "        global last_frame1\n",
    "        last_frame1 = frame1.copy()\n",
    "        pic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(pic)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        lmain.imgtk = imgtk\n",
    "        lmain.configure(image=imgtk)\n",
    "        root.update()\n",
    "        lmain.after(10, show_subject)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        exit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "W tej metodzie odczytujemy wartość emocji, którą chcemy przekonwertować na RGB. Otwwieramy odpowiadający obrazek oraz ładujemy go do etykiety interfejsu. Aktualizujemy katalog główny oraz wywołujemy tą samą funkcję opóźnioną o 10 milisekund."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_avatar():\n",
    "    frame2 = cv2.imread(emoji_dist[show_text[0]])\n",
    "    pic2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "    img2 = Image.fromarray(frame2)\n",
    "    imgtk2 = ImageTk.PhotoImage(image=img2)\n",
    "    lmain2.imgtk2 = imgtk2\n",
    "    lmain3.configure(text=emotion_dict[show_text[0]], font=('arial', 45, 'bold'))\n",
    "    lmain2.configure(image=imgtk2)\n",
    "    root.update()\n",
    "    lmain2.after(10, show_avatar)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inicjujemy ramkę główną oraz potrzebne labele, które będą zawierały załadowe zdjęcia lub film oraz ustawiemy parametru interfejsu programu. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    frame_number = 0\n",
    "    root = tk.Tk()\n",
    "    lmain = tk.Label(master=root, padx=50, bd=10)\n",
    "    lmain2 = tk.Label(master=root, bd=10)\n",
    "    lmain3 = tk.Label(master=root, bd=10, fg=\"#CDCDCD\", bg=\"black\")\n",
    "    lmain.pack(side=LEFT)\n",
    "    lmain.place(x=50, y=250)\n",
    "    lmain3.pack()\n",
    "    lmain3.place(x=960, y=250)\n",
    "    lmain2.pack()\n",
    "    lmain2.place(x=960, y=350)\n",
    "\n",
    "    root.title(\"Photo to Emoji\")\n",
    "    root.geometry(\"1400x900+100+10\")\n",
    "    root['bg'] = 'black'\n",
    "    Button(root, text=\"Quit\", fg=\"red\", command=root.destroy, font=(\"arial\", 25, \"bold\")).pack(side=BOTTOM)\n",
    "    threading.Thread(target=show_subject).start()\n",
    "    threading.Thread(target=show_avatar).start()\n",
    "    root.mainloop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}